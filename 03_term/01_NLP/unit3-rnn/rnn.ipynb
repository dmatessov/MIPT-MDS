{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df_train = pd.read_csv('data/train.csv', index_col=0)\n",
    "df_prediction = pd.read_csv('data/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 41159 entries, 0 to 41156\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       41158 non-null  object\n",
      " 1   Sentiment  41155 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 964.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3798 entries, 787bc85b-20d4-46d8-84a0-562a2527f684 to 8d09ea68-a130-4f3a-8777-f821b354542d\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    3798 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 59.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_prediction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем строки с пропущенными метками классов\n",
    "df_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={'Sentiment': 'Y', 'Text': 'X'})\n",
    "df_prediction = df_prediction.rename(columns={'Text': 'X'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X                   Y\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1  advice Talk to your neighbours family to excha...            Positive\n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3  My food stock is not the only one which is emp...            Positive\n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y\n",
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7711\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим балланс классов\n",
    "df_train.Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока только отметим что выборку нельзя назвать сбалансированной (разница более чем 2 раза). Потом можно попробовать сбалансировать ее для улучшения результата, но для начала возьмем как есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# кодируем признки\n",
    "# для начала попробуем Lable Encoding\n",
    "df_train.Y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_map = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive' :4}\n",
    "target_classes = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
    "df_train.Y.replace(cls_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Токенезируем тексты\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "    for dataset in datasets:\n",
    "        for index, row in dataset.iterrows():\n",
    "            yield (TOKENIZER(row['X']))\n",
    "\n",
    "# отправляем в токенизатор также df_prediction на тот случай, если там есть слова, которые не встречались в df_train\n",
    "# хотя, в этом случае для них все равно не получится определить ebeddings\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([train_df, test_df, df_prediction]), min_freq=1, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93, 2135, 13, 606, 9, 0] ['some', 'text', 'for', 'test', 'of', 'tokenezation']\n"
     ]
    }
   ],
   "source": [
    "# проверим что получилось\n",
    "tokens = TOKENIZER('some text for test of tokenezation')\n",
    "indexes = vocab(tokens)\n",
    "print(indexes, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        features = sample['X']  # Assuming 'X' is your features column\n",
    "        target = sample['Y']  # Assuming 'Y' is your target column\n",
    "        return  target, features\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "max_words = 40\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(TOKENIZER(text)) for text in X]\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
    "    return torch.tensor(X, dtype=torch.int32, device='cpu'), torch.tensor(Y, device='cpu') \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in train_dataset:\n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 40]) torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_loader:\n",
    "    print(X.shape, Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define RNN Classification Network\n",
    "\n",
    "In this section, we have created a neural network that we'll be using for the text classification task. The network consists of 3 layers.\n",
    "\n",
    "Embeddings layer\n",
    "\n",
    "RNN layer\n",
    "\n",
    "Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "embed_len = 50\n",
    "hidden_dim = 50\n",
    "n_layers=1\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
    "        self.rnn = nn.RNN(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, len(target_classes))\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        embeddings = self.embedding_layer(X_batch)\n",
    "        output, hidden = self.rnn(embeddings, torch.randn(n_layers, len(X_batch), hidden_dim))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (embedding_layer): Embedding(97236, 50)\n",
       "  (rnn): RNN(50, 50, batch_first=True)\n",
       "  (linear): Linear(in_features=50, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_classifier = RNNClassifier()\n",
    "rnn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer : Embedding(97236, 50)\n",
      "Parameters : \n",
      "torch.Size([97236, 50])\n",
      "\n",
      "Layer : RNN(50, 50, batch_first=True)\n",
      "Parameters : \n",
      "torch.Size([50, 50])\n",
      "torch.Size([50, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([50])\n",
      "\n",
      "Layer : Linear(in_features=50, out_features=5, bias=True)\n",
      "Parameters : \n",
      "torch.Size([5, 50])\n",
      "torch.Size([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for layer in rnn_classifier.children():\n",
    "    print(\"Layer : {}\".format(layer))\n",
    "    print(\"Parameters : \")\n",
    "    for param in layer.parameters():\n",
    "        print(param.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rnn_classifier(torch.randint(0, len(vocab), (1024, max_words)))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        Y_shuffled, Y_preds, losses = [],[],[]\n",
    "        for X, Y in val_loader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_shuffled.append(Y)\n",
    "            Y_preds.append(preds.argmax(dim=-1))\n",
    "\n",
    "        Y_shuffled = torch.cat(Y_shuffled)\n",
    "        Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "        print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        print(\"Valid Acc  : {:.3f}\".format(accuracy_score(Y_shuffled.detach().numpy(), Y_preds.detach().numpy())))\n",
    "\n",
    "\n",
    "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    for i in range(1, epochs+1):\n",
    "        losses = []\n",
    "        for X, Y in tqdm(train_loader):\n",
    "            Y_preds = model(X)\n",
    "\n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
    "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.584\n",
      "Valid Loss : 1.568\n",
      "Valid Acc  : 0.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.548\n",
      "Valid Loss : 1.541\n",
      "Valid Acc  : 0.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.531\n",
      "Valid Loss : 1.541\n",
      "Valid Acc  : 0.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.523\n",
      "Valid Loss : 1.534\n",
      "Valid Acc  : 0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.524\n",
      "Valid Loss : 1.543\n",
      "Valid Acc  : 0.292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.508\n",
      "Valid Loss : 1.533\n",
      "Valid Acc  : 0.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.502\n",
      "Valid Loss : 1.514\n",
      "Valid Acc  : 0.301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.488\n",
      "Valid Loss : 1.522\n",
      "Valid Acc  : 0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.471\n",
      "Valid Loss : 1.528\n",
      "Valid Acc  : 0.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.451\n",
      "Valid Loss : 1.516\n",
      "Valid Acc  : 0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.418\n",
      "Valid Loss : 1.507\n",
      "Valid Acc  : 0.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.384\n",
      "Valid Loss : 1.518\n",
      "Valid Acc  : 0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.360\n",
      "Valid Loss : 1.512\n",
      "Valid Acc  : 0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.324\n",
      "Valid Loss : 1.517\n",
      "Valid Acc  : 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.283\n",
      "Valid Loss : 1.593\n",
      "Valid Acc  : 0.319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.267\n",
      "Valid Loss : 1.563\n",
      "Valid Acc  : 0.313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.224\n",
      "Valid Loss : 1.558\n",
      "Valid Acc  : 0.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.198\n",
      "Valid Loss : 1.617\n",
      "Valid Acc  : 0.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.176\n",
      "Valid Loss : 1.541\n",
      "Valid Acc  : 0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.137\n",
      "Valid Loss : 1.598\n",
      "Valid Acc  : 0.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.115\n",
      "Valid Loss : 1.597\n",
      "Valid Acc  : 0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.084\n",
      "Valid Loss : 1.635\n",
      "Valid Acc  : 0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.046\n",
      "Valid Loss : 1.748\n",
      "Valid Acc  : 0.322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 1.030\n",
      "Valid Loss : 1.657\n",
      "Valid Acc  : 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.999\n",
      "Valid Loss : 1.740\n",
      "Valid Acc  : 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.982\n",
      "Valid Loss : 1.713\n",
      "Valid Acc  : 0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.956\n",
      "Valid Loss : 1.710\n",
      "Valid Acc  : 0.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.936\n",
      "Valid Loss : 1.769\n",
      "Valid Acc  : 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.906\n",
      "Valid Loss : 1.816\n",
      "Valid Acc  : 0.336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:04<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.908\n",
      "Valid Loss : 1.740\n",
      "Valid Acc  : 0.322\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "rnn_classifier = RNNClassifier()\n",
    "optimizer = Adam(rnn_classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "TrainModel(rnn_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
