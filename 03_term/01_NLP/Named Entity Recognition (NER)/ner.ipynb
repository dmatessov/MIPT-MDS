{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Packages and Import Dependencies\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device index: 0\n",
      "Device name: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check CUDA Availability and Device Information\n",
    "# Disable CUDA\n",
    "# torch.backends.cudnn.enabled = False\n",
    "# torch.cuda.is_available = lambda : False\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device index:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     # os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Read and Prepare Data\n",
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"r\", encoding='utf8') as f:\n",
    "        content = f.read().strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = read_conll_file(\"data/hw/train.txt\")\n",
    "validation_data = read_conll_file(\"data/hw/dev.txt\")\n",
    "test_data = read_conll_file(\"data/hw/test.txt\")\n",
    "\n",
    "\n",
    "def convert_to_dataset(data, label_map, max_sample_len=None):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence]\n",
    "        ner_tags = [label_map[token_data[1]] for token_data in sentence]\n",
    "        formatted_data[\"tokens\"].append(tokens[:max_sample_len])\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags[:max_sample_len])\n",
    "    return Dataset.from_dict(formatted_data)\n",
    "\n",
    "\n",
    "label_list = sorted(list(set([token_data[1] for sentence in train_data for token_data in sentence])))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "\n",
    "train_dataset = convert_to_dataset(train_data, label_map, max_sample_len=200)\n",
    "validation_dataset = convert_to_dataset(validation_data, label_map, max_sample_len=200)\n",
    "test_dataset = convert_to_dataset(test_data, label_map, max_sample_len=200)\n",
    "\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 239/239 [00:00<00:00, 545kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 175/175 [00:00<00:00, 553kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 382k/382k [00:00<00:00, 1.10MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 464kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 41.6M/41.6M [00:15<00:00, 2.75MB/s]\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Initialize Tokenizer and Model (https://huggingface.co/DeepPavlov)\n",
    "model_name = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"#\"DeepPavlov/distilrubert-tiny-cased-conversational\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Metrics and Tokenization Function\n",
    "\n",
    "def compute_metrics(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"classification_report\": classification_report(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7746 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7746/7746 [00:00<00:00, 9838.94 examples/s] \n",
      "Map: 100%|██████████| 2582/2582 [00:00<00:00, 9014.83 examples/s]\n",
      "Map: 100%|██████████| 2582/2582 [00:00<00:00, 5929.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Tokenize Datasets and Set Training Arguments\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4, #8,\n",
    "    per_device_eval_batch_size=4, #8,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Data Collator and Initialize Trainer\n",
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
    "\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 107/1937 [00:02<00:51, 35.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3798, 'learning_rate': 4.7418688693856486e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 205/1937 [00:05<00:52, 33.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2192, 'learning_rate': 4.4837377387712956e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 307/1937 [00:08<00:44, 36.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1294, 'learning_rate': 4.225606608156944e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 406/1937 [00:11<00:40, 37.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1442, 'learning_rate': 3.967475477542592e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 500/1937 [00:13<00:41, 35.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1396, 'learning_rate': 3.70934434692824e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 26%|██▌       | 500/1937 [00:20<00:41, 35.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10930658876895905, 'eval_precision': 0.7558702276393618, 'eval_recall': 0.809249664171944, 'eval_f1': 0.7816496756255793, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.79      0.83      0.81      1393\\n         ORG       0.69      0.65      0.67      1679\\n         PER       0.78      0.92      0.84      2139\\n\\n   micro avg       0.76      0.81      0.78      5211\\n   macro avg       0.75      0.80      0.77      5211\\nweighted avg       0.75      0.81      0.78      5211\\n', 'eval_runtime': 6.3495, 'eval_samples_per_second': 406.647, 'eval_steps_per_second': 101.741, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 607/1937 [00:23<00:37, 35.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1159, 'learning_rate': 3.4512132163138876e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 706/1937 [00:26<00:35, 34.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1027, 'learning_rate': 3.193082085699535e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 804/1937 [00:29<00:32, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0943, 'learning_rate': 2.9349509550851833e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 904/1937 [00:31<00:27, 37.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1, 'learning_rate': 2.6768198244708313e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 1000/1937 [00:34<00:26, 35.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0842, 'learning_rate': 2.4186886938564793e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 1000/1937 [00:40<00:26, 35.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09546869993209839, 'eval_precision': 0.7938834600072385, 'eval_recall': 0.8418729610439455, 'eval_f1': 0.8171742572413152, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.86      0.85      0.86      1393\\n         ORG       0.72      0.70      0.71      1679\\n         PER       0.80      0.94      0.87      2139\\n\\n   micro avg       0.79      0.84      0.82      5211\\n   macro avg       0.80      0.83      0.81      5211\\nweighted avg       0.79      0.84      0.81      5211\\n', 'eval_runtime': 6.402, 'eval_samples_per_second': 403.311, 'eval_steps_per_second': 100.906, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1104/1937 [00:44<00:24, 34.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0837, 'learning_rate': 2.160557563242127e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1205/1937 [00:47<00:21, 34.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0698, 'learning_rate': 1.902426432627775e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1306/1937 [00:49<00:16, 37.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0752, 'learning_rate': 1.644295302013423e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1404/1937 [00:52<00:14, 38.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0935, 'learning_rate': 1.3861641713990709e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1500/1937 [00:55<00:12, 34.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0587, 'learning_rate': 1.1280330407847187e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 77%|███████▋  | 1500/1937 [01:01<00:12, 34.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08500814437866211, 'eval_precision': 0.8232054341839544, 'eval_recall': 0.8604874304356169, 'eval_f1': 0.8414336648526929, 'eval_classification_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.84      0.90      0.87      1393\\n         ORG       0.75      0.72      0.73      1679\\n         PER       0.87      0.94      0.90      2139\\n\\n   micro avg       0.82      0.86      0.84      5211\\n   macro avg       0.82      0.86      0.84      5211\\nweighted avg       0.82      0.86      0.84      5211\\n', 'eval_runtime': 6.3915, 'eval_samples_per_second': 403.974, 'eval_steps_per_second': 101.072, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1605/1937 [01:05<00:09, 34.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0856, 'learning_rate': 8.699019101703666e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1706/1937 [01:07<00:07, 32.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0695, 'learning_rate': 6.117707795560145e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1804/1937 [01:10<00:03, 33.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0728, 'learning_rate': 3.536396489416624e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1906/1937 [01:13<00:00, 34.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0818, 'learning_rate': 9.550851832731028e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1937/1937 [01:14<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 74.5427, 'train_samples_per_second': 103.914, 'train_steps_per_second': 25.985, 'train_loss': 0.11499161626730128, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1937, training_loss=0.11499161626730128, metrics={'train_runtime': 74.5427, 'train_samples_per_second': 103.914, 'train_steps_per_second': 25.985, 'train_loss': 0.11499161626730128, 'epoch': 1.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Train the Model\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
