{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7sfWBeBq8Wx"
      },
      "source": [
        "## DQN\n",
        "\n",
        "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9txpm2ZBSpWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055d0f2f-19df-4ccf-c46e-af90228209fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "\n",
        "    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
        "    !pip -q install piglet\n",
        "    !pip -q install imageio_ffmpeg\n",
        "    !pip -q install moviepy==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rsaCPHqSpWl"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgjQwIsSpWl"
      },
      "source": [
        "<img src=\"https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRnOxiAZrOFN",
        "outputId": "0bfb978d-a2ca-487f-876c-32957f60a2a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0463569 , -0.03709729, -0.00314481, -0.02675435], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbIV7w42Fp1"
      },
      "source": [
        "Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
        "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7HyNGNDSpWm"
      },
      "source": [
        "Будем приближать Q-функцию агента, минимизируя среднеквадратичную TD-ошибку:\n",
        "$$\n",
        "\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')] \\\\\n",
        "L = \\frac{1}{N} \\sum_i \\delta_i^2,\n",
        "$$\n",
        "где\n",
        "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n",
        "* $\\gamma$ дисконтирующий множитель.\n",
        "\n",
        "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. В статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFkc4eN16Lh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNqTerAgSpWm",
        "outputId": "c2ce1d1e-5daf-476b-b235-f89215bf24e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action_space: 2 \n",
            "State_space: (4,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "print(f'Action_space: {n_actions} \\nState_space: {env.observation_space.shape}')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ihv7jcsSpWn"
      },
      "source": [
        "Задавайте небольшой размер скрытых слоев, например не больше 200.\n",
        "Определяем граф вычислений:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3JPKemCSpWn"
      },
      "outputs": [],
      "source": [
        "# TODO: refactor hidden_dims and make it more clear (typing and so on)\n",
        "def create_network(input_dim, hidden_dims, output_dim):\n",
        "    # network = nn.Sequential(\n",
        "    #    torch.nn.Linear(input_dim, ...),\n",
        "    #    torch.nn.ReLU(),\n",
        "    #    ...\n",
        "    # )\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    network = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dims[0]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims[1], output_dim),\n",
        "    )\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "    return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbzigcFfSpWn"
      },
      "outputs": [],
      "source": [
        "def select_action_eps_greedy(network, state, epsilon):\n",
        "    \"\"\"Выбирает действие epsilon-жадно.\"\"\"\n",
        "    if not isinstance(state, torch.Tensor):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "    Q_s = network(state).detach().numpy()\n",
        "\n",
        "    # action =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    if epsilon < np.random.random():\n",
        "        action = np.argmax(Q_s)\n",
        "    else:\n",
        "        n_actions = Q_s.shape[-1]\n",
        "        action = np.random.choice(n_actions)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    action = int(action)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN5c4jwoSpWn"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(\n",
        "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
        "):\n",
        "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
        "\n",
        "    # переводим входные данные в тензоры\n",
        "    states = torch.tensor(np.array(states), dtype=torch.float32)    # shape: [batch_size, state_size]\n",
        "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
        "\n",
        "\n",
        "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32) # shape: [batch_size, state_size]\n",
        "    is_done = torch.tensor(is_done, dtype=torch.bool)    # shape: [batch_size]\n",
        "\n",
        "    # получаем значения q для всех действий из текущих состояний\n",
        "    predicted_qvalues = network(states)\n",
        "\n",
        "    # получаем q-values для выбранных действий\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
        "\n",
        "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
        "    # predicted_next_qvalues =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    predicted_next_qvalues = network(next_states)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
        "    # next_state_values =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    next_state_values = torch.max(predicted_next_qvalues.detach(), axis=-1)[0]\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    assert next_state_values.dtype == torch.float32\n",
        "\n",
        "    # вычисляем target q-values для функции потерь\n",
        "    #  target_qvalues_for_actions =\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    # для последнего действия в эпизоде используем\n",
        "    # упрощенную формулу Q(s,a) = r(s,a),\n",
        "    # т.к. s' для него не существует\n",
        "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
        "\n",
        "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
        "\n",
        "    # MSE loss для минимизации\n",
        "    loss = torch.mean(losses)\n",
        "    # добавляем регуляризацию на значения Q\n",
        "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim(\n",
        "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
        "        assert next_state_values.data.dim(\n",
        "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
        "        assert target_qvalues_for_actions.data.dim(\n",
        "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
        "\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_g2MOsSpWn"
      },
      "source": [
        "## Simple DQN\n",
        "\n",
        "Немного модифицированная версия кода, запускающего обучение Q-learning из прошлой тетрадки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5K15RZvSpWn"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, network, opt, t_max=300, epsilon=0, train=False):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "            loss, _ = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooQJI-IrSpWn"
      },
      "outputs": [],
      "source": [
        "def test_dqn():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _ = generate_session(env, network, opt, epsilon=eps, train=True)\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew = generate_session(env, network, opt, epsilon=eps, train=False)\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nZ86IJVSpWo",
        "outputId": "eb9aca1e-3a74-4aeb-871b-72cf3fc082d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #49\tmean reward = 8.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 10.333\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 11.250\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 10.800\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 11.200\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 11.000\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 10.200\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 10.000\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 10.200\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 11.200\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 15.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 16.600\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 24.200\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 26.000\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 28.200\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 27.200\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 30.000\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 27.400\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 28.200\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 27.800\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 47.600\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 44.800\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 71.600\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 69.600\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 69.600\tepsilon = 0.037\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f8c3e3aff6c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-bea0702af3e5>\u001b[0m in \u001b[0;36mtest_dqn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_schedule\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-fb82ff381192>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(env, network, opt, t_max, epsilon, train)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mterminated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-cfadddc62606>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m(network, states, actions, rewards, next_states, is_done, gamma, check_shapes, regularizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# next_state_values =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m\"\"\"<codehere>\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_next_qvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"</codehere>\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "test_dqn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIHHO_2xSpWo"
      },
      "source": [
        "## DQN with Experience Replay\n",
        "\n",
        "Теперь попробуем добавить поддержку памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', done)\\}$.\n",
        "\n",
        "Тогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyhisdP3SpWo"
      },
      "outputs": [],
      "source": [
        "def sample_batch(replay_buffer, n_samples):\n",
        "    # sample randomly `n_samples` samples from replay buffer\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples)\n",
        "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        dones.append(done)\n",
        "\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ShcuDhGileLH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN_h8192SpWo"
      },
      "outputs": [],
      "source": [
        "def generate_session_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "            replay_buffer.append((s, a, r, next_s, terminated and not truncated))\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if replay_buffer and glob_step % train_schedule == 0:\n",
        "                # sample new batch: train_batch = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                train_batch = sample_batch(replay_buffer, batch_size)\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r34ncGGYSpWo"
      },
      "source": [
        "После проверки скорости обучения можете поэкспериментировать с различными `train_schedule`, `batch_size`, а также с размером буфера `replay_buffer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-oAOXalSpWo"
      },
      "outputs": [],
      "source": [
        "def test_dqn_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jpkpTlXSpWp",
        "outputId": "55e51e61-348a-4128-f608-8269271ff4d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.000\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.000\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.000\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 9.800\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 10.200\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 10.000\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 10.600\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 13.400\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 13.600\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 15.200\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 22.000\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 24.800\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 25.200\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 35.200\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 39.400\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 42.200\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 41.600\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 85.800\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 98.800\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 120.800\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 161.200\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 163.600\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 127.600\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 112.800\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 142.800\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 93.200\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 88.200\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 117.200\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 166.200\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 129.400\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 150.600\tepsilon = 0.020\n",
            "Epoch: #1649\tmean reward = 180.200\tepsilon = 0.018\n",
            "Epoch: #1699\tmean reward = 173.600\tepsilon = 0.017\n",
            "Epoch: #1749\tmean reward = 149.600\tepsilon = 0.015\n",
            "Epoch: #1799\tmean reward = 153.400\tepsilon = 0.014\n",
            "Epoch: #1849\tmean reward = 168.600\tepsilon = 0.012\n",
            "Epoch: #1899\tmean reward = 171.000\tepsilon = 0.011\n",
            "Epoch: #1949\tmean reward = 167.000\tepsilon = 0.010\n",
            "Epoch: #1999\tmean reward = 169.400\tepsilon = 0.009\n",
            "Epoch: #2049\tmean reward = 202.400\tepsilon = 0.008\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_replay_buffer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hP5L_ChSpWp"
      },
      "source": [
        "## DQN with Prioritized Experience Replay\n",
        "\n",
        "Добавим каждому переходу, хранящемуся в памяти, значение приоритета. Популярным вариантом является абсолютное значение TD-ошибки.\n",
        "\n",
        "Однако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз - накладно. Приходится искать баланс между точностью и скоростью.\n",
        "\n",
        "Здесь мы будем делать следующее:\n",
        "\n",
        "- использовать TD-ошибку в кач-ве приоритета\n",
        "- после использования батча при обучении, обновляем значения приоритета для этого батча в памяти\n",
        "- будем периодически сортировать память для того, чтобы новые переходы заменяли собой те переходы, у которых наименьшие значения ошибки (т.е. наименьший приоритет)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq7TgsnLSpWp"
      },
      "outputs": [],
      "source": [
        "def softmax(xs, temp=1000.):\n",
        "    if not isinstance(xs, np.ndarray):\n",
        "        xs = np.array(xs)\n",
        "\n",
        "    # Обрати внимание, насколько большая температура по умолчанию!\n",
        "    exp_xs = np.exp((xs - xs.max()) / temp)\n",
        "    return exp_xs / exp_xs.sum()\n",
        "\n",
        "def sample_prioritized_batch(replay_buffer, n_samples):\n",
        "    # Sample randomly `n_samples` samples from replay buffer weighting by priority (sample's loss)\n",
        "    # and split an array of samples into arrays: states, actions, rewards, next_actions, dones\n",
        "    # Also, keep samples' indices (into `indices`) to return them too!\n",
        "    \"\"\"<codehere>\"\"\"\n",
        "    losses = [sample[0] for sample in replay_buffer]\n",
        "    probs = softmax(losses)\n",
        "    indices = np.random.choice(len(replay_buffer), n_samples, p=probs)\n",
        "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
        "    for i in indices:\n",
        "        _, s, a, r, n_s, done = replay_buffer[i]\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        next_actions.append(n_s)\n",
        "        dones.append(done)\n",
        "    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "    batch = np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)\n",
        "    return batch, indices\n",
        "\n",
        "def update_batch(replay_buffer, indices, batch, new_losses):\n",
        "    \"\"\"Updates batches with corresponding indices replacing their loss value.\"\"\"\n",
        "    states, actions, rewards, next_states, is_done = batch\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        new_batch = new_losses[i], states[i], actions[i], rewards[i], next_states[i], is_done[i]\n",
        "        replay_buffer[indices[i]] = new_batch\n",
        "\n",
        "def sort_replay_buffer(replay_buffer):\n",
        "    \"\"\"Sorts replay buffer to move samples with lesser loss to the beginning\n",
        "    ==> they will be replaced with the new samples earlier.\"\"\"\n",
        "    new_rb = deque(maxlen=replay_buffer.maxlen)\n",
        "    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n",
        "    return new_rb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujAVrSilSpWp"
      },
      "outputs": [],
      "source": [
        "def generate_session_prioritized_rb(\n",
        "        env, network, opt, replay_buffer, glob_step,\n",
        "        train_schedule, batch_size,\n",
        "        t_max=300, epsilon=0, train=False\n",
        "):\n",
        "    \"\"\"генерация сессии и обучение\"\"\"\n",
        "    total_reward = 0\n",
        "    s, _ = env.reset()\n",
        "    epsilon = epsilon if train else 0.\n",
        "\n",
        "    for t in range(t_max):\n",
        "        a = select_action_eps_greedy(network, s, epsilon=epsilon)\n",
        "        next_s, r, terminated, truncated, _ = env.step(a)\n",
        "\n",
        "        if train:\n",
        "            # Compute new sample loss (it's the second returning value - `losses` - from compute_td_loss)\n",
        "            # we need `losses.numpy()[0]`\n",
        "            with torch.no_grad():\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                _, losses = compute_td_loss(network, [s], [a], [r], [next_s], [terminated and not truncated])\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # put new sample into replay_buffer\n",
        "            \"\"\"<codehere>\"\"\"\n",
        "            replay_buffer.append((losses.numpy()[0], s, a, r, next_s, terminated and not truncated))\n",
        "            \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % train_schedule == 0:\n",
        "                # sample new batch: train_batch, indices = ...\n",
        "                \"\"\"<codehere>\"\"\"\n",
        "                train_batch, indices = sample_prioritized_batch(replay_buffer, batch_size)\n",
        "                \"\"\"</codehere>\"\"\"\n",
        "                states, actions, rewards, next_states, is_done = train_batch\n",
        "\n",
        "                opt.zero_grad()\n",
        "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # compute updated losses for the training batch and update batch in replay buffer\n",
        "                    \"\"\"<codehere>\"\"\"\n",
        "                    _, losses = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
        "                    update_batch(replay_buffer, indices, train_batch, losses.numpy())\n",
        "                    \"\"\"</codehere>\"\"\"\n",
        "\n",
        "            # periodically re-sort replay buffer to prioritize replacing with new samples those samples\n",
        "            # that have the least loss\n",
        "            if len(replay_buffer) >= batch_size and (glob_step + 1) % 25*train_schedule == 0:\n",
        "                replay_buffer = sort_replay_buffer(replay_buffer)\n",
        "\n",
        "        glob_step += 1\n",
        "        total_reward += r\n",
        "        s = next_s\n",
        "        if terminated:\n",
        "            break\n",
        "\n",
        "    return total_reward, glob_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLEirszhSpWq"
      },
      "outputs": [],
      "source": [
        "def test_dqn_prioritized_replay_buffer():\n",
        "    lr = .0001\n",
        "    eps, eps_decay = .5, .998\n",
        "    train_ep_len, eval_schedule = 10000, 50\n",
        "    train_schedule, batch_size = 4, 32\n",
        "    replay_buffer = deque(maxlen=4000)\n",
        "    eval_rewards = deque(maxlen=5)\n",
        "    glob_step = 0\n",
        "\n",
        "    env.reset()\n",
        "    network = create_network(env.observation_space.shape[0], [128, 128], env.action_space.n)\n",
        "    opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(train_ep_len):\n",
        "        _, glob_step = generate_session_prioritized_rb(\n",
        "            env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
        "        )\n",
        "\n",
        "        if (ep + 1) % eval_schedule == 0:\n",
        "            ep_rew, _ = generate_session_prioritized_rb(\n",
        "                env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
        "            )\n",
        "            eval_rewards.append(ep_rew)\n",
        "            running_avg_rew = np.mean(eval_rewards)\n",
        "            print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
        "\n",
        "            if eval_rewards and running_avg_rew >= 200.:\n",
        "                print(\"Принято!\")\n",
        "                break\n",
        "\n",
        "        eps *= eps_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7leQWfTwSpWq",
        "outputId": "1f196538-fafe-420d-c71c-9a5343efbcb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: #49\tmean reward = 9.000\tepsilon = 0.453\n",
            "Epoch: #99\tmean reward = 9.500\tepsilon = 0.410\n",
            "Epoch: #149\tmean reward = 9.667\tepsilon = 0.371\n",
            "Epoch: #199\tmean reward = 9.750\tepsilon = 0.336\n",
            "Epoch: #249\tmean reward = 10.200\tepsilon = 0.304\n",
            "Epoch: #299\tmean reward = 10.000\tepsilon = 0.275\n",
            "Epoch: #349\tmean reward = 13.000\tepsilon = 0.249\n",
            "Epoch: #399\tmean reward = 15.200\tepsilon = 0.225\n",
            "Epoch: #449\tmean reward = 19.000\tepsilon = 0.204\n",
            "Epoch: #499\tmean reward = 21.000\tepsilon = 0.184\n",
            "Epoch: #549\tmean reward = 36.400\tepsilon = 0.167\n",
            "Epoch: #599\tmean reward = 45.200\tepsilon = 0.151\n",
            "Epoch: #649\tmean reward = 66.200\tepsilon = 0.136\n",
            "Epoch: #699\tmean reward = 120.400\tepsilon = 0.123\n",
            "Epoch: #749\tmean reward = 120.800\tepsilon = 0.112\n",
            "Epoch: #799\tmean reward = 115.200\tepsilon = 0.101\n",
            "Epoch: #849\tmean reward = 113.400\tepsilon = 0.091\n",
            "Epoch: #899\tmean reward = 101.000\tepsilon = 0.083\n",
            "Epoch: #949\tmean reward = 56.600\tepsilon = 0.075\n",
            "Epoch: #999\tmean reward = 65.400\tepsilon = 0.068\n",
            "Epoch: #1049\tmean reward = 68.600\tepsilon = 0.061\n",
            "Epoch: #1099\tmean reward = 82.000\tepsilon = 0.055\n",
            "Epoch: #1149\tmean reward = 86.200\tepsilon = 0.050\n",
            "Epoch: #1199\tmean reward = 90.600\tepsilon = 0.045\n",
            "Epoch: #1249\tmean reward = 98.400\tepsilon = 0.041\n",
            "Epoch: #1299\tmean reward = 111.600\tepsilon = 0.037\n",
            "Epoch: #1349\tmean reward = 146.200\tepsilon = 0.034\n",
            "Epoch: #1399\tmean reward = 149.200\tepsilon = 0.030\n",
            "Epoch: #1449\tmean reward = 144.000\tepsilon = 0.027\n",
            "Epoch: #1499\tmean reward = 136.200\tepsilon = 0.025\n",
            "Epoch: #1549\tmean reward = 128.000\tepsilon = 0.022\n",
            "Epoch: #1599\tmean reward = 83.800\tepsilon = 0.020\n",
            "Epoch: #1649\tmean reward = 98.200\tepsilon = 0.018\n",
            "Epoch: #1699\tmean reward = 119.200\tepsilon = 0.017\n",
            "Epoch: #1749\tmean reward = 165.600\tepsilon = 0.015\n",
            "Epoch: #1799\tmean reward = 170.200\tepsilon = 0.014\n",
            "Epoch: #1849\tmean reward = 177.400\tepsilon = 0.012\n",
            "Epoch: #1899\tmean reward = 182.400\tepsilon = 0.011\n",
            "Epoch: #1949\tmean reward = 194.600\tepsilon = 0.010\n",
            "Epoch: #1999\tmean reward = 181.400\tepsilon = 0.009\n",
            "Epoch: #2049\tmean reward = 217.200\tepsilon = 0.008\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "test_dqn_prioritized_replay_buffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7mx89OQSpWq",
        "outputId": "85f05f83-7a1f-41c2-cfc4-aa315dc184bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved in:  render/dqn.ipynb\n",
            "Saved in:  render/dqn-solution.ipynb\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'</comment>'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"<comment>\"\"\"\n",
        "from codehere import convert\n",
        "import os\n",
        "\n",
        "os.makedirs('render', exist_ok=True)\n",
        "\n",
        "nb_name = 'dqn'\n",
        "convert(\n",
        "    file=f'{nb_name}.ipynb',\n",
        "    outfile=f'render/{nb_name}.ipynb',\n",
        "    clear=True, replacement=\" Здесь ваш код \"\n",
        ")\n",
        "convert(\n",
        "    file=f'{nb_name}.ipynb',\n",
        "    outfile=f'render/{nb_name}-solution.ipynb',\n",
        "    clear=True, solution=True, replacement=\" Здесь ваш код \"\n",
        ")\n",
        "\"\"\"</comment>\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}